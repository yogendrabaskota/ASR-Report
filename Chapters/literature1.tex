\chapter{LITERATURE REVIEW}
Supriya Khadka, Ranju G.C., Prabin Paudel, Rahul Shah, Basanta Joshi, Nepali Text-to-Speech Synthesis using Tacotron2 for Melspectrogram Generation \cite{first}, Proposes generating high quality Nepali speech using the Tacotron2 model and HiFiGAN/WaveGlow vocoders. This method, which preprocesses text and fine-tunes the Tacotron2 model, achieved a record Mean Opinion Score of 4.03 for naturalness in Nepali Text-to-Speech tasks.  
\par


Basanta joshi, Bharat Bhatta, Sanjeeb Prasad panday, Ram Krishna Maharjan, A Novel Deep Learning Based Nepali Speech Recognition \cite{inbook} (2022),  Proposes a Nepali speech recognition model using CNN-GRU with data from Librispeech. MFCC extracts features, CNN-GRU develops the acoustic model, and CTC decodes. Performance is evaluated using Word Error Rate. 



\par



Manish Dhakal, Arman Chhetri, Aman Kumar Gupta, Prabin Lamichhane, Suraj Pandey, Subarna Shakya, Automatic speech recognition for the Nepali language using CNN, bidirectional LSTM and ResNet \cite{inproceedings} (2022),  Introduces an end-to-end deep learning ASR model for transcribing Nepali speech to text, trained on the OpenSLR dataset. After preprocessing to 
remove silent gaps, MFCCs are used as audio features. The best performance, with a character error rate of 17.06\%, is achieved using a Bidirectional LSTM paired with ResNet and a one dimensional CNN, employing CTC for loss calculation and beam search decoding. 



\par


Paribesh Regmi, Arjun Dahal, Basanta Joshi, Nepali Speech Recognition using RNN-CTC Model \cite{link4} (2019), introduces a Nepali Speech Recognition model using RNNs and CTC. The RNN processes sequential audio data, and CTC maximizes the probability of desired labels from RNN output. The model outputs Nepali text and uses a character set of 67 Nepali characters for transcription. 


\par

Soumya Sen, Anjan Dutta, Nilanjan Dey, Speech Processing and Recognition System: Concepts, Techniques and Research Overviews \cite{link5} (2019), Reviews how speech recognition systems work, including key ideas like understanding sounds and predicting words. It looks at methods such as Hidden Markov Models (HMMs), Deep Neural Networks (DNNs), Recurrent Neural Networks (RNNs), and Connectionist Temporal Classification (CTC). It also highlights recent improvements in making these systems more accurate and efficient, especially with new models that handle multiple types of data, deal with noise, and work in real-time. 